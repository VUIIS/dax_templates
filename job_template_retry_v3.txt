#!/bin/bash
#SBATCH --mail-user=${job_email}
#SBATCH --mail-type=${job_email_options}
#SBATCH --account=${job_rungroup}
#SBATCH --nodes=1
#SBATCH --ntasks=${job_ppn}
#SBATCH --time=${job_walltime}
#SBATCH --mem=${job_memory}
#SBATCH -o ${job_output_file}

#echo "Starting smemwatch"
#smemwatch -k 95 -d 50 $$$$ &

uname -a # outputs node info (name, date&time, type, OS, etc)
date

#=============================================================================
#VERSION=
#JOBDIR=
#INDIR=
#OUTDIR=
#DSTDIR=
#INLIST=
#OUTLIST=
#CONTAINERPATH=
#MAINCMD=
#XNATHOST=
#XNATUSER=
${job_cmds}
#=============================================================================
SESSLIMIT=25
SLEEPMAX=300

echo $DSTDIR
echo $INDIR
echo $OUTDIR
echo $CONTAINERPATH
echo $XNATHOST
echo $XNATUSER
echo Max sessions: $SESSLIMIT
echo Max random delay between tries: $SLEEPMAX


# Make opening sessions robust to intermittent failure
# Note, the sessid is passed out via an echo so we can't also generate
# any logging messages
function robust_session {
    for try in {1..3}; do
        SESSID=$(curl -s -n "$XNATHOST/data/JSESSION")
        if [[ -n "${SESSID}" ]]; then 
            echo "${SESSID}"
            break
        fi
    done
}


# Make curl commands robust to intermittent failure
# Here we do not need to pass anything back, so we can have logging messages
function robust_curl {
    local sessid="${1}"
    local curlcmd="${2}"
	safecmd=$(echo ${curlcmd} | sed "s/$sessid/SESSID/g")
    for try in {1..3}; do
        echo "Trying command (attempt ${try}):"
        echo $safecmd
        eval result=\$\($curlcmd\)
        echo "Result=$result"        
        if [[ "$result" == '200' ]]; then
            break
        fi
    done
    if [[ "$result" != '200' ]]; then
        echo CURL COMMAND FAILED ALL ATTEMPTS
        exit 1
    fi
}


# Random initial wait that always happens, to avoid a lot of jobs started
# simultaneously by the cluster hitting xnat with requests all at once
SLEEPSECS=$[ ( $RANDOM % $SLEEPMAX ) ]s
echo "Initial delay of $SLEEPSECS"
sleep $SLEEPSECS


# Check number of open sessions on host before we download
echo "Checking for low enough open sessions on XNAT"
while true;do
    SESSID=$(robust_session)
    if [[ -z "${SESSID}" ]]; then
        echo FAILED TO GET A SESSION ID IN ALL ATTEMPTS
        exit 1
    fi
    SESSCOUNT=`curl -s -b "JSESSIONID=$SESSID" "$XNATHOST/data/user/$XNATUSER/sessions" | cut -s -f2 -d ":" | cut -f1 -d "}"`
    echo "$SESSCOUNT"
    if [ -z "$SESSCOUNT" ]; then
        echo -n "Could not get session count, sleeping "
        curl -s -b "JSESSIONID=$SESSID" -X DELETE "$XNATHOST/data/JSESSION"
        SLEEPSECS=$[ ( $RANDOM % $SLEEPMAX ) ]s && echo "$SLEEPSECS" && sleep $SLEEPSECS
    elif (( "$SESSCOUNT" > "$SESSLIMIT" )); then
        echo -n "Cannot download yet, too many open sessions, sleeping "
        curl -s -b "JSESSIONID=$SESSID" -X DELETE "$XNATHOST/data/JSESSION"
        SLEEPSECS=$[ ( $RANDOM % $SLEEPMAX ) ]s && echo "$SLEEPSECS" && sleep $SLEEPSECS
    else
        echo "Session looks good, ready to download" 
        break
    fi
done
echo "Working session established"


# Set up working dir structure
mkdir -p $INDIR
mkdir -p $OUTDIR


# Collect inputs
for IN in "${INLIST[@]}"; do
    IFS=',' read -r col1 col2 col3 <<< "$IN"
    if [ $col2 == "FILE" ]; then
        CMD="curl -D - -s -b JSESSIONID=$SESSID \"$col3\" -o $INDIR/$col1"
        CMD+=" | head -n 1 | awk '{print \$2}'"
    elif [ $col2 == "DIRJ" ]; then
        CMD="curl -D - -s -b JSESSIONID=$SESSID \"$col3\"?format=zip -o $INDIR/${col1}.zip"
        CMD+=" | head -n 1 | awk '{print \$2}'"
        CMD+=" && unzip -q -j $INDIR/${col1}.zip -d $INDIR/$col1"
    else
        CMD="curl -D - -s -b JSESSIONID=$SESSID \"${col3}?format=zip&structure=simplified\" -o $INDIR/${col1}.zip"
        CMD+=" | head -n 1 | awk '{print \$2}'" 
        CMD+=" && unzip -q $INDIR/${col1}.zip -d $INDIR/$col1 && mv $INDIR/$col1/*/out/* $INDIR/$col1"
    fi

    # Run the command robustly
    robust_curl $SESSID "$CMD"
    
done

# Disconnect XNAT session
curl -s -b "JSESSIONID=$SESSID" -X DELETE "$XNATHOST/data/JSESSION"


# Run main command
echo $_JAVA_OPTIONS
echo $MAINCMD
eval $MAINCMD

# Write version files
mkdir -p $DSTDIR
echo $VERSION > $DSTDIR/version.txt
sha256sum $CONTAINERPATH | awk '{print $1}' > $DSTDIR/dax_docker_version.txt

# Convert ps to pdf
if [ -e $OUTDIR/report.ps ]; then
    gs -sDEVICE=pdfwrite -o $OUTDIR/report.pdf $OUTDIR/report.ps
fi

# Handle outputs
errors=0
haspdf=0
for OUT in "${OUTLIST[@]}"; do
    IFS=',' read -r col1 col2 col3 col4 <<< "$OUT"
    pathlist=(${OUTDIR}/${col1})
    
    # Check for missing output
    if [[ ${#pathlist[@]} == 1 && ! -e $pathlist ]]; then
        if [[ $col4 != "F" ]]; then errors=1 && echo "ERROR:missing required output:$col1"
        else echo "WARN:output not required"; fi
        continue
    fi

    # Copy output based on type
    mkdir -p "$DSTDIR/$col3"
    if [ $col3 == "PDF" ]; then
        if [ $col2 != "FILE" ]; then errors=1 && echo "ERROR:illegal type for PDF";
        elif [[ ${#pathlist[@]} != 1 ]]; then errors=1 && echo "ERROR:multiple PDFs";
        else haspdf=1 && cp $OUTDIR/$col1 $DSTDIR/$col3 || errors=1; fi
    elif [ $col2 == "FILE" ]; then cp $OUTDIR/$col1 $DSTDIR/$col3 || errors=1;
    elif [ $col2 == "DIR" ]; then cp -r $OUTDIR/$col1/. $DSTDIR/$col3 || errors=1;
    else errors=1 && echo "ERROR:invalid type:$col2"; fi
done

if [ $errors -gt 0 ]; then echo "JOB_FAILED" && touch $DSTDIR/JOB_FAILED.txt;
else echo "COMPLETE" && touch $DSTDIR/READY_TO_UPLOAD.txt; fi

rm -rf $INDIR $OUTDIR

echo "DONE!"
